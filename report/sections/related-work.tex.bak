%! TEX root = ../report.tex

\chapter{Related work}\label{chp:related}
    \section{\mbox{Overview}}
        % RFC675 overview
        When TCP was originally proposed in December 1974, by \citeauthor{rfc675} in RFC 675~\cite{rfc675}, flow diagrams and implementation suggestions were defined but lacked specific pseudo-code routines that could be implemented directly, were not provided. At that time TCP was young and naive due to having little exposure to real-world use cases. Furthermore, the limitations of the technology of the era were apparent in the original design given the scope and proposed use of the protocol, compared to the significantly higher speed demands in 2017.

        % RFC793 overview & level-ip implementation to the spec
        RFC 793 followed some years later and provided a more specific breakdown, that can be directly translated and implemented in code, of each event within the protocol and an appropriate procedure to handle it~\cite[Page~54-77]{rfc793}. \citeauthor{rfc793} who published the specification, now recognised as an official internet standard, defined TCP as `a connection-oriented, end-to-end reliable protocol' as it is now widely known. The document provides a well-defined list of requirements for the protocol to run, as well as a multitude of provisions for successful operation within the promises.
        Many implementations of the TCP/IP stack including, but not limited to, `Level-IP' by \citeauthor{levelip-spec} follows this specification~\cite[\texttt{src/tcp\_input.c} line~262]{levelip-spec} very closely which, in theory, produces a TCP that should interwork seamlessly, as per the specification, with any other correctly implemented TCP. % chktex 13

        % lwIP goals
        Not all TCP/IP stacks are created equal; there are numerous incentives for developing alternate implementations, for example `lwIP' from \citeauthor{lwip}~(\citeyear{lwip}) was built `to reduce memory usage and code size, making lwIP suitable for use in small clients with very limited resources such as embedded systems'. There are many inefficiencies in `standard' networking stacks like those included in popular operating systems such as Linux, BSD, macOS and Windows, to name a few, especially regarding memory usage. These protocols make the assumption that the physical system has considerable amounts of memory available for receiving, processing and duplicating network data both in the kernel and in user applications.

        \citeauthor{lwip}~\cite{lwip} makes the opposite assumption and as a result produced a system where minimal replication of data and little wasted memory allocation occurs. Using dynamically sized packet buffers \texttt{pbufs}, \citeauthor{lwip} made efficient use of RAM, ROM and pooled memory to address network data without requiring it to be copied to a local storage space before being actioned. Through many small optimisations like these, lwIP was, and still is, a very effective network stack, usable on even the most restricted hardware which in the growing interconnected embedded device market is invaluable.
        % TODO: Maybe talk about uIP here, if I have space/time/the will to live

        % mTCP goals
        Conversely, some alternate implementations exist for quite the opposite reasons such as `mTCP' \citeauthor{jeong2014mtcp}, which was constructed as `a highly scalable user-level TCP stack for multicore systems'. The intention was for mTCP to outperform competing solutions in packet throughput and data volume. According to their claims, mTCP can surpass Linux by a factor of 25 in `small message transactions' while also boosting regular performance of popular applications between 33\% and 320\%. Such performance numbers are impressive, especially considering the widespread use of Linux for commercial applications and hosting, which begs the question: ``Why is it significantly faster than the default Linux implementation and why hasn't Linux caught up yet?''~\cite[2.2, 3]{jeong2014mtcp}

        Many of the improvements suggested by \citeauthor{jeong2014mtcp}~\cite{jeong2014mtcp} are very intelligent applications of high-speed network adapters and multicore processor systems, such as servers. Any TCP network stack that is to be run in these kinds of scenarios should consider these optimisations for improved performance. It is likely that many of the proposed enhancements would also benefit low-power single CPU systems too, with the most probable outcome being reduced latency and memory usage.

    \section{Demultiplexing TCP}\label{sec:demultiplex}
        \citeauthor{braun:inria-00074040} designed a modified BSD TCP/IP stack where the IP layer resides in the kernel and TCP is split in two between kernel and userspace into TCPU and TCPK respectively. TCP processing is moved mostly into the user region, residing as local code in the calling process. The only exception to this is the `demultiplexing' step, TCPK, where TCP frames are routed to the correct user program based on the ports and addresses from the IP layer packet, providing security for the receiving process.

        % Demultiplexing in userspace, or not
        \citeauthor{braun:inria-00074040} theorised that a potential alternative method for demultiplexing packets would be to pass all incoming traffic directly into a userspace daemon for processing, removing the requirement on kernel modifications. However, it is concluded that this concept is impractical and inefficient compared with the alternative solution (above) as packets are processed by two userspace applications, causing more context switches passing data from the daemon to the receiving processes~\cite[2.1]{braun:inria-00074040}\cite[3]{edwards1995experiences}. Generally, this assumption of relative inefficiency is true, however, in certain circumstances there can be situations where this is actually a practical and viable solution. This project aims to implement a usermode-only networking stack, meaning kernel modifications are not plausible and therefore this solution is ideal when optimised appropriately, reducing the context switching overhead.

    \section{Throughput performance}\label{sec:thruput}
        Much of the research surrounding TCP, particularly that providing userspace implementation detail, is focussed on optimising the protocol for high performance and high throughput. \citeauthor{edwards1995experiences}, in \citeyear{edwards1995experiences} demonstrated throughput of 160 Mbit/s using a userspace TCP implementation running over \textit{coaxial} \textit{token-ring} \textit{ATM} networking, making use of the solid existing HP-UX kernel TCP code along with single-copy from the NIC to the user application using shared memory in the TCP stack and the application. This particular arrangement managed 80\% of the performance of the existing single-copy TCP stack and 150\% of the double-copy kernel stack. \citeauthor{braun:inria-00074040} in \citeyear{braun:inria-00074040} yielded similar results of around 40-50\%~\cite[5]{braun:inria-00074040} between the default kernel and TCPU/TCPK stacks running on much less powerful hardware. % chktex 8
        % TODO: Talk about lwIP here (2,6,12)
        %     Copying from kernel <-> user

        % mTCP thread-local storage and multicore
        \citeauthor{jeong2014mtcp} shows that with the use of multiple receive queues spread across individual CPU cores and fewer locks along with improved buffer management and fewer context switches between kernel and user mode their implementation can yield a much improved throughput compared to Linux and other user-mode concepts. Much of the improved performance is thanks to the use of \textit{thread local storage} of socket buffers, TCP buffer pools and other data structures relating to individual threads that are not shared. Greatly reduced usage of locks across threads, reducing \textit{Lock contention}, helps to minimise idle time in both incoming and outgoing packet processing.

    \section{Reducing data copying overhead}\label{sec:reducecopy}
        A common theme across many protocol implementations is the focus on reducing overhead due to data copying. Any copied data is potentially wasted time, especially if the same functionality could be programmed using a \textit{zero-copy} approach, where incoming packets are deposited directly from the network adapter into the user program without being duplicated one or more times. Many of the related works include some variation of a transmit/receive queue utilised by every layer from the NIC to the user program, minimal or zero data copying. \citeauthor{jeong2014mtcp} utilised an event-driven packet I/O library to divide incoming batches of packets into multiple queues written directly from the network interface and passed directly through into the user TCP~\cite[3.1]{jeong2014mtcp}.

        \citeauthor{braun:inria-00074040} took a different approach to reducing data copying where typically packet buffers are copied twice: once from the device to an input buffer queue then secondly into the application buffer. In certain situations, by the process of \textit{header prediction}, the kernel copies the header data only. Then assuming the predicted header size was correct, enclosed packet data is copied directly into the user program receive buffer~\cite[4.1]{braun:inria-00074040}. By modern standards this is less than optimal, as now there exists suitable methods to skip copying entirely and use memory-mapped buffers.

        Due to the limitations of the BSD socket API, \textit{zero-copy} networking is not trivially implemented without some significant changes to how data is copied into the user memory, although there is one solution that harnesses the power of hardware offload. Using a feature of virtual memory to remap \textit{memory pages}, data can be offloaded by the network interface directly into aligned \textit{memory pages}, separating header from the payload~\cite[2.3]{chase2001end}. Specific hardware and driver support is required for this, and even more complex connection tracking is required for TCP offload. A result of the \textit{memory page} mapping is large contiguous blocks of memory containing packet payloads produced without a single data copy.

    \section{TCP security considerations}
        By moving processing from kernel to userspace, protections such as obscurity from other programs is reduced. \citeauthor{braun:inria-00074040} raised the issue that with a userspace TCP process, data for multiple connections is loaded into the memory space of a single process, enabling it to be viewed or modified by any privileged program~\cite[1, 2.1]{braun:inria-00074040}. In most cases this is not a significant issue, however, it should still be considered. The only plausible solution is to move the demultiplexing stage back into the kernel to restore the connection security.
